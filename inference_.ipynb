{"cells":[{"cell_type":"markdown","metadata":{"id":"i-tTvEF1RT3y"},"source":["## Setup\n","\n","Run the cells below to setup and install the required libraries. For our experiment we will need `accelerate`, `peft`, `transformers`, `datasets` and TRL to leverage the recent [`SFTTrainer`](https://huggingface.co/docs/trl/main/en/sft_trainer). We will use `bitsandbytes` to [quantize the base model into 4bit](https://huggingface.co/blog/4bit-transformers-bitsandbytes). We will also install `einops` as it is a requirement to load Falcon models."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-22T18:58:21.165575Z","iopub.status.busy":"2023-11-22T18:58:21.165219Z","iopub.status.idle":"2023-11-22T18:59:17.327466Z","shell.execute_reply":"2023-11-22T18:59:17.326490Z","shell.execute_reply.started":"2023-11-22T18:58:21.165541Z"},"id":"mNnkgBq7Q3EU","outputId":"806d7490-f248-48cd-df34-9e5d6519e336","trusted":true},"outputs":[],"source":["%pip install  -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n","%pip install  datasets bitsandbytes einops wandb"]},{"cell_type":"markdown","metadata":{"id":"rjOMoSbGSxx9"},"source":["## Loading the model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-22T18:59:23.042627Z","iopub.status.busy":"2023-11-22T18:59:23.042264Z","iopub.status.idle":"2023-11-22T19:04:21.761062Z","shell.execute_reply":"2023-11-22T19:04:21.760003Z","shell.execute_reply.started":"2023-11-22T18:59:23.042593Z"},"trusted":true},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n","from accelerate import infer_auto_device_map\n","model_name = \"codellama/CodeLlama-13b-Instruct-hf\"\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16,\n","    bnb_4bit_use_double_quant=False,\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    trust_remote_code=True,\n","    device_map=\"auto\",\n","    offload_folder='offload'\n",")"]},{"cell_type":"markdown","metadata":{"id":"xNqIYtQcUBSm"},"source":["Let's also load the tokenizer below"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-22T19:04:32.609125Z","iopub.status.busy":"2023-11-22T19:04:32.608520Z","iopub.status.idle":"2023-11-22T19:04:34.323295Z","shell.execute_reply":"2023-11-22T19:04:34.322055Z","shell.execute_reply.started":"2023-11-22T19:04:32.609091Z"},"id":"XDS2yYmlUAD6","trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-22T19:04:36.240504Z","iopub.status.busy":"2023-11-22T19:04:36.240137Z","iopub.status.idle":"2023-11-22T19:04:36.248901Z","shell.execute_reply":"2023-11-22T19:04:36.247979Z","shell.execute_reply.started":"2023-11-22T19:04:36.240473Z"},"trusted":true},"outputs":[],"source":["example = {\n","    'question': \n","        \"Can you solve this real interview question? Find The Original Array of Prefix Xor - You are given an integer array pref of size n. Find and return the array arr of size n that satisfies:\\n\\n * pref[i] = arr[0] ^ arr[1] ^ ... ^ arr[i].\\n\\nNote that ^ denotes the bitwise-xor operation.\\n\\nIt can be proven that the answer is unique.\\n\\n \\n\\nExample 1:\\n\\n\\nInput: pref = [5,2,0,3,1]\\nOutput: [5,7,2,3,2]\\nExplanation: From the array [5,7,2,3,2] we have the following:\\n- pref[0] = 5.\\n- pref[1] = 5 ^ 7 = 2.\\n- pref[2] = 5 ^ 7 ^ 2 = 0.\\n- pref[3] = 5 ^ 7 ^ 2 ^ 3 = 3.\\n- pref[4] = 5 ^ 7 ^ 2 ^ 3 ^ 2 = 1.\\n\\n\\nExample 2:\\n\\n\\nInput: pref = [13]\\nOutput: [13]\\nExplanation: We have pref[0] = arr[0] = 13.\\n\\n\\n \\n\\nConstraints:\\n\\n * 1 <= pref.length <= 105\\n * 0 <= pref[i] <= 106\",\n","    \"code\": \"class Solution {\\npublic:\\n    vector<int> findArray(vector<int>& pref) {\\n        int n = pref.size();\\n        \\n        vector<int> arr;\\n        arr.push_back(pref[0]);\\n        for (int i = 1; i < n; i++) {\\n            arr.push_back(pref[i] ^ pref[i - 1]);\\n        }\\n        \\n        return arr;\\n    }\\n};\"\n","}  # Replace 0 with the index of the example you want to use\n","text = f\"\"\"\n","    <s>[INST] <<SYS>>\n","    {{You are a smart AI Explainer. You will explain what is happening in the ###SOLUTION based on what is asked in the ###QUESTION and add the explaination after the ###EXPLANATION }}\n","    <</SYS>>\n","    ###QUESTION: {example['question']} \n","    ###SOLUTION: {example['code']}[/INST]\n","    \n","    ###EXPLANATION:\"\"\"\n","text"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-22T19:04:40.325640Z","iopub.status.busy":"2023-11-22T19:04:40.324612Z","iopub.status.idle":"2023-11-22T19:05:12.588680Z","shell.execute_reply":"2023-11-22T19:05:12.587640Z","shell.execute_reply.started":"2023-11-22T19:04:40.325598Z"},"id":"pgt86z-x4diG","trusted":true},"outputs":[],"source":["device = \"cuda:0\"\n","\n","# Tokenize the input text\n","inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n","\n","# Generate the output using the model\n","outputs = model.generate(**inputs, max_new_tokens=200)\n","\n","# Decode and print the output\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4030425,"sourceId":7010261,"sourceType":"datasetVersion"},{"datasetId":4032407,"sourceId":7013339,"sourceType":"datasetVersion"},{"datasetId":4032502,"sourceId":7013495,"sourceType":"datasetVersion"}],"dockerImageVersionId":30528,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
